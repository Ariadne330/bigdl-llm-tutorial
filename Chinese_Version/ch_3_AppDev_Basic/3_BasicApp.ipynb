{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: 快速开始\n",
    "\n",
    "此 notebook 介绍了`bigdl-llm`的基本用法，并带您逐步构建一个最基础的聊天应用程序。\n",
    "\n",
    "## 3.1 安装 `bigdl-llm`\n",
    "\n",
    "如果您尚未安装bigdl-llm，请按照以下示例进行安装。这一行命令将安装最新版本的`bigdl-llm`以及所有常见LLM应用程序开发所需的依赖项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --pre --upgrade bigdl-llm[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 加载预训练模型\n",
    "\n",
    "在使用LLM之前，您首先需要加载一个模型。这里我们以一个相对较小的LLM作为示例，即[open_llama_3b_v2](https://huggingface.co/openlm-research/open_llama_3b_v2)。\n",
    "\n",
    "## 3.3 优化模型\n",
    "\n",
    "通常情况下，您只需要一行`optimize_model`就可以轻松优化已加载的任何PyTorch模型。\n",
    "\n",
    "模型加载过程如下：\n",
    "\n",
    "首先，使用您喜欢的任何PyTorch API来加载您的模型。这里，我们使用`Hugging Face Transformers`库的`LlamaForCausalLM`来加载`open_llama_3b_v2`。\n",
    "\n",
    "然后，调用`optimize_model`来优化已加载的模型（默认采用INT4优化）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM\n",
    "from bigdl.llm import optimize_model\n",
    "\n",
    "model_path = 'openlm-research/open_llama_3b_v2'\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(model_path,\n",
    "                                         torch_dtype=\"auto\",\n",
    "                                         low_cpu_mem_usage=True)\n",
    "\n",
    "# With only one line to enable BigDL-LLM optimization on model\n",
    "model = optimize_model(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **注意**\n",
    ">\n",
    "> * 如果您希望使用除了INT4以外的精度（比如INT3/INT5/INT8等），或者想了解更多关于函数参数和使用方法，请参考[API文档](https://bigdl.readthedocs.io/en/latest/doc/PythonAPI/LLM/optimize.html#).\n",
    ">\n",
    "> * [open_llama_3b_v2](https://huggingface.co/openlm-research/open_llama_3b_v2)作为在huggingface上开源的大语言模型，`openlm-research/open_llama_3b_v2`是它在huggingface上的model_id。`from_pretrained`函数会默认从huggingface上下载模型、缓存到本地路径（比如 `~/.cache/huggingface`）并加载。下载模型的过程可能会较久，您也可以自行下载模型，再将`model_path`变量修改为本地路径。关于`from_pretrained`的用法，请参考[这里](https://huggingface.co/docs/transformers/main_classes/model#transformers.PreTrainedModel.from_pretrained)。\n",
    "\n",
    "\n",
    "### 3.2.2 保存和加载优化后的模型\n",
    "\n",
    "在上一节中，在调用`optimize_model`之前，使用`Huggingface transformers` API加载的模型实际上是以FP16精度加载的。这可能会导致相当大的内存使用，尤其是对于较大尺寸的模型，在加载过程中可能会出现内存不足的错误。\n",
    "\n",
    "为了解决这个问题，`bigdl-llm`允许使用`save_low_bit`保存优化后的低精度模型，并在之后使用`load_low_bit`加载用于推理。这种方法不需要加载原始的FP16模型，既节省了内存，又提高了加载速度。而且，由于优化后的模型格式与平台无关，您可以在各种不同操作系统的计算机上无缝执行保存和加载操作。这种灵活性使您可以在内存更大的服务器上进行优化和保存操作，然后在有限内存的个人电脑上部署模型进行推理应用。\n",
    "\n",
    "**保存优化后模型**\n",
    "\n",
    "例如，您可以使用`save_low_bit`函数来保存优化后模型，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = './open-llama-3b-v2-bigdl-llm-INT4'\n",
    "\n",
    "model.save_low_bit(save_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**加载优化后模型**\n",
    "\n",
    "您可以使用`load_low_bit`函数加载优化后的模型，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.llm.optimize import load_low_bit\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(model_path,\n",
    "                                         torch_dtype=\"auto\",\n",
    "                                         low_cpu_mem_usage=True)\n",
    "model = load_low_bit(model, save_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 （可选）使用Transformers-Style API加载模型\n",
    "\n",
    "optimize_model API可用于优化任何PyTorch模型，无论模型是使用哪种API或库来加载的。此外，`bigdl-llm`还提供了另一套API，专为Hugging Face Transformers模型设计，称为`transformers-style API`。\n",
    "\n",
    "例如，您可以使用`bigdl.llm.transformers.AutoModelForCausalLM`来加载`open_llama_3b_v2`。在`from_pretrained`中指定`load_in_4bit=True`将在加载过程中自动应用INT4优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.llm.transformers import AutoModelForCausalLM  # note that the AutoModelForCausalLM here is imported from bigdl.llm.transformers\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                             load_in_4bit=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **注意**\n",
    ">\n",
    "> 更多关于transformers-style API的使用方法，请参考[API 文档](https://bigdl.readthedocs.io/en/latest/doc/PythonAPI/LLM/transformers.html#)。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 构建一个最简单的聊天应用\n",
    "\n",
    "现在，模型已经成功加载，可以开始构建我们的第一个聊天应用程序了。接下来将使用`Hugginface transformers`推理API来完成这个任务。\n",
    "\n",
    "\n",
    "> **注意**\n",
    "> \n",
    "> 本节中的代码完全使用`Huggingface transformers` API实现。`bigdl-llm`不需要在推理代码中进行任何更改，因此您可以在推理阶段使用任何库来构建您的应用程序。\n",
    "\n",
    "\n",
    "> **注意**\n",
    "> \n",
    "> 我们使用了Q&A的对话式模板，以更好地回答问题。\n",
    "\n",
    "\n",
    "> **注意**\n",
    "> \n",
    "> 您在调用`generate`函数时，可以通过修改`max_new_tokens`参数来指定要预测的tokens数目上限。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: xxxx s\n",
      "-------------------- Output --------------------\n",
      "Q: What is CPU?\n",
      "A: CPU stands for Central Processing Unit. It is the brain of the computer.\n",
      "Q: What is RAM?\n",
      "A: RAM stands for Random Access Memory.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.inference_mode():\n",
    "    prompt = 'Q: What is CPU?\\nA:'\n",
    "    \n",
    "    # tokenize the input prompt from string to token ids\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    # predict the next tokens (maximum 32) based on the input token ids\n",
    "    output = model.generate(input_ids, max_new_tokens=32)\n",
    "    # decode the predicted token ids to output string\n",
    "    output_str = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    print('-'*20, 'Output', '-'*20)\n",
    "    print(output_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
