{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: 应用开发基础\n",
    "\n",
    "此 Notebook 将向您展示如何安装 `bigdl-llm`、加载预训练的大语言模型 (LLM) 并且实现基础的对话应用。\n",
    "\n",
    "## 3.1 安装 `bigdl-llm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --pre --upgrade bigdl-llm[all]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一行命令将会安装 `bigdl-llm` 以及用于 LLM 应用程序开发的所有依赖项。\n",
    "\n",
    "\n",
    "## 3.2 加载预训练模型\n",
    "\n",
    "现在让我们加载一个相对较小的 LLM 模型，即 [open_llama_3b_v2](https://huggingface.co/openlm-research/open_llama_3b_v2). \n",
    "\n",
    "\n",
    "### 3.2.1 优化模型\n",
    "\n",
    "您可以用[Hugging Face Transformers](https://huggingface.co/docs/transformers/index)加载\n",
    "`open_llama_3b_v2`模型，然后只需要加上一行来使用`bigdl-llm`的 Pytorch API，从而实现INT4优化来加速`open_llama_3b_v2`，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM\n",
    "from bigdl.llm import optimize_model\n",
    "\n",
    "model_path = 'openlm-research/open_llama_3b_v2'\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(model_path,\n",
    "                                         torch_dtype=\"auto\",\n",
    "                                         low_cpu_mem_usage=True)\n",
    "\n",
    "# With only one line to enable BigDL-LLM optimization on model\n",
    "model = optimize_model(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **注意**\n",
    ">\n",
    "> * 关于`optimize_model`的使用方法，请参考[API document](https://bigdl.readthedocs.io/en/latest/doc/PythonAPI/LLM/optimize.html#).\n",
    ">\n",
    "> * [open_llama_3b_v2](https://huggingface.co/openlm-research/open_llama_3b_v2)作为在huggingface上开源的大语言模型，`openlm-research/open_llama_3b_v2`是它在huggingface上的model_id。`from_pretrained`函数会默认从huggingface上下载模型、缓存到本地路径（比如 `~/.cache/huggingface`）并加载。下载模型的过程可能会较久，您也可以自行下载模型，再将`model_path`变量修改为本地路径。关于`from_pretrained`的用法，请参考[这里](https://huggingface.co/docs/transformers/main_classes/model#transformers.PreTrainedModel.from_pretrained)。\n",
    "\n",
    "\n",
    "### 3.2.2 保存并加载优化后模型\n",
    "\n",
    "为了加快加载过程，您也可以在第一次加载并优化模型后使用`save_low_bit`函数来保存。如此，在之后的使用中，您可以通过`load_low_bit`函数直接加载优化后模型，加载过程相对更快。需要强调的是，对优化后模型的保存与加载可以在不同的机器上进行。\n",
    "\n",
    "**保存优化后模型**\n",
    "\n",
    "我们可以使用`save_low_bit`函数来保存优化后模型，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = './open-llama-3b-v2-bigdl-llm-INT4'\n",
    "\n",
    "model.save_low_bit(save_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**加载优化后模型**\n",
    "\n",
    "那么，为了加载优化后模型，我们提供了`load_low_bit`函数，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.llm.optimize import load_low_bit\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(model_path,\n",
    "                                         torch_dtype=\"auto\",\n",
    "                                         low_cpu_mem_usage=True)\n",
    "model = load_low_bit(model, save_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 （可选）使用Transformers-Style API加载模型\n",
    "\n",
    "除了上述介绍的Pytorch API，`bigdl-llm`也提供了transformers-style API以对任意 Hugging Face *Transformers* 模型进行INT4优化，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.llm.transformers import AutoModelForCausalLM  # note that the AutoModelForCausalLM here is imported from bigdl.llm.transformers\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                             load_in_4bit=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **注意**\n",
    ">\n",
    "> 关于transformers-style API的使用方法，请参考[API document](https://bigdl.readthedocs.io/en/latest/doc/PythonAPI/LLM/transformers.html#)。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 运行 LLM\n",
    "\n",
    "现在，您可以直接使用`transformers` API进行模型推理，从而实现基本的对话应用。\n",
    "\n",
    "\n",
    "> **注意**\n",
    "> \n",
    "> 我们使用了Q&A的对话式模板，以更好地回答问题。\n",
    "\n",
    "\n",
    "> **注意**\n",
    "> \n",
    "> 您在调用`generate`函数时，可以通过修改`max_new_tokens`参数来指定要预测的tokens数目上限。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: xxxx s\n",
      "-------------------- Output --------------------\n",
      "Q: What is CPU?\n",
      "A: CPU stands for Central Processing Unit. It is the brain of the computer.\n",
      "Q: What is RAM?\n",
      "A: RAM stands for Random Access Memory.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.inference_mode():\n",
    "    prompt = 'Q: What is CPU?\\nA:'\n",
    "    \n",
    "    # tokenize the input prompt from string to token ids\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    # predict the next tokens (maximum 32) based on the input token ids\n",
    "    output = model.generate(input_ids, max_new_tokens=32)\n",
    "    # decode the predicted token ids to output string\n",
    "    output_str = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    print('-'*20, 'Output', '-'*20)\n",
    "    print(output_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.4 后续学习\n",
    "\n",
    "在下一章中，我们将体验到大语言模型对多种语言的支持。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
