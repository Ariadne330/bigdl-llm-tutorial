{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Basic Application Development\n",
    "\n",
    "This notebook introduces the essential usage of `bigdl-llm`, and walks you through building a very basic chat application.\n",
    "\n",
    "## 3.1 Install `bigdl-llm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't installed `bigdl-llm`, install it as shown below. The one-line command will install the latest `bigdl-llm` with all the dependencies for common LLM application development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --pre --upgrade bigdl-llm[all]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Load a pretrained Model\n",
    "\n",
    "Before using a LLM, you need to first load one. Here we take a relatively small LLM, i.e. [open_llama_3b_v2](https://huggingface.co/openlm-research/open_llama_3b_v2) as an example.\n",
    "\n",
    "### 3.2.1 Optimize Model\n",
    "\n",
    "In general, you just need one-line `optimize_model` to optimize any PyTorch model you have loaded. \n",
    "\n",
    "The models loading process is as follows. \n",
    "\n",
    "First, use any PyTorch API's you like to load your model. Here we use [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) library `LlamaForCausalLM` to load `open_llama_3b_v2`. \n",
    "\n",
    "Then, call `optimize_model` to optimize the loaded model (by default INT4 optimization is applied). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM\n",
    "from bigdl.llm import optimize_model\n",
    "\n",
    "model_path = 'openlm-research/open_llama_3b_v2'\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(model_path,\n",
    "                                         torch_dtype=\"auto\",\n",
    "                                         low_cpu_mem_usage=True)\n",
    "\n",
    "# With only one line to enable BigDL-LLM optimization on model\n",
    "model = optimize_model(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**\n",
    ">\n",
    "> * If you want to use precisions other than INT4(e.g. NF4/INT5/INT8,etc.), or know more details about the arguments, please refer to [API document](https://bigdl.readthedocs.io/en/latest/doc/PythonAPI/LLM/optimize.html#) for more information. \n",
    ">\n",
    "> * [open_llama_3b_v2](https://huggingface.co/openlm-research/open_llama_3b_v2) is a pretrained large language model hosted on huggingface. `openlm-research/open_llama_3b_v2` is its huggingface model_id. `LlamaForCausalLM.from_pretrained` will automatically download the model from huggingface to a local cache path (e.g. `~/.cache/huggingface`) and load the model. It may take a long time to download the model using API. You can also download the model yourself, and set `model_path` to the local path of the downloaded model. More information about `from_pretrained` can be found [here](https://huggingface.co/docs/transformers/main_classes/model#transformers.PreTrainedModel.from_pretrained).\n",
    "\n",
    "\n",
    "### 3.2.2 Save & Load Optimized Model\n",
    "\n",
    "In the previous section, prior to calling `optimize_model`, the model - loaded using the Huggingface transformers API - is actually in FP16 precision. This can already cause substantial memory usage, potentially leading to Out-of-Memory errors during the loading process, especially for larger models.\n",
    "\n",
    "To address this problem, `bigdl-llm` allows you to save the optimized low-bit model using `save_low_bit` and load it later with `load_low_bit` for inference. This approach bypasses the need to load the original FP16 model, saving both memory and time. Moreover, because the optimized model format is platform-agnostic, you can seamlessly perform saving and loading operations across various machines, regardless of their operating systems. This flexibility enables you to perform optimization/saving on a high-RAM server and deploy the model for inference on a PC with limited RAM.\n",
    "\n",
    "\n",
    "**Save Optimized Model**\n",
    "\n",
    "For exmaple, you can use the `save_low_bit` function to save the optimized model as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = './open-llama-3b-v2-bigdl-llm-INT4'\n",
    "\n",
    "model.save_low_bit(save_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Optimized Model**\n",
    "\n",
    "Then use `load_low_bit` to load the optimized low-bit model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.llm.optimize import load_low_bit\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(model_path,\n",
    "                                         torch_dtype=\"auto\",\n",
    "                                         low_cpu_mem_usage=True)\n",
    "model = load_low_bit(model, save_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 (Optional) Load Model with Transformers-Style API\n",
    "\n",
    "The `optimize_model` API can be used to optimize any PyTorch model, regardless of the loading API or library employed. Additionally, `bigdl-llm` provides another set of API for Hugging Face *Transformers* models, referred to as the transformers-style API. \n",
    "\n",
    "For example, you can use `bigdl.llm.transformers.AutoModelForCausalLM` to load `open_llama_3b_v2`. Specify `load_in_4bit` in `from_pretrained` will automaticlaly apply INT4 optimziations during the loading process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.llm.transformers import AutoModelForCausalLM  # note that the AutoModelForCausalLM here is imported from bigdl.llm.transformers\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                             load_in_4bit=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**\n",
    ">\n",
    "> Please refer to [API document](https://bigdl.readthedocs.io/en/latest/doc/PythonAPI/LLM/transformers.html#) for more information about transformers-style API."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Building a Simple Chat Application\n",
    "\n",
    "Now that the model is successfully loaded, we can start building our very first chat application. We shall use the `Hugginface transformers` inference API to do this job.\n",
    "\n",
    "> **Note**\n",
    "> \n",
    "> The code in this section is solely implemented using `Huggingface transformers` API. `bigdl-llm` does not require any change in the inference code so you can use any libaries to build your appliction at inference stage.  \n",
    "\n",
    "> **Note**\n",
    "> \n",
    "> Here we use Q&A dialog prompt template so that it can answer our questions.\n",
    "\n",
    "\n",
    "> **Note**\n",
    "> \n",
    "> `max_new_tokens` parameter in the `generate` function defines the maximum number of tokens to predict. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Output --------------------\n",
      "Q: What is CPU?\n",
      "A: CPU stands for Central Processing Unit. It is the brain of the computer.\n",
      "Q: What is RAM?\n",
      "A: RAM stands for Random Access Memory.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.inference_mode():\n",
    "    prompt = 'Q: What is CPU?\\nA:'\n",
    "    \n",
    "    # tokenize the input prompt from string to token ids\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    # predict the next tokens (maximum 32) based on the input token ids\n",
    "    output = model.generate(input_ids, max_new_tokens=32)\n",
    "    # decode the predicted token ids to output string\n",
    "    output_str = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    print('-'*20, 'Output', '-'*20)\n",
    "    print(output_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
