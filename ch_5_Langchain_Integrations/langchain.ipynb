{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5: Langchain Integrations \n",
    "\n",
    "[LangChain](https://python.langchain.com/docs/get_started/introduction.html) is a popular library for developing applications powered by language models. You can use LangChain with LLMs to build various interesting applications such as [Chatbot](https://github.com/intel-analytics/BigDL/blob/main/python/llm/example/langchain/transformers_int4/chat.py), [Document Q&A](https://github.com/intel-analytics/BigDL/blob/main/python/llm/example/langchain/transformers_int4/docqa.py), [voice assistant](https://github.com/intel-analytics/BigDL/blob/main/python/llm/example/langchain/transformers_int4/voiceassistant.py). BigDL-LLM provides LangChain integrations (i.e. LLM wrappers and embeddings) and you can use them the same way as [other LLM wrappers in LangChain](https://python.langchain.com/docs/integrations/llms/). \n",
    "\n",
    "This notebook goes over how to use langchain to interact with BigDL-LLM.\n",
    "\n",
    "## 5.1 Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, install BigDL-LLM in your prepared environment. For best practices of environment setup, refer to [Chapter 2]() in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bigdl-llm[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then install LangChain. - (TODO: Verify which langchain version works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain==0.0.184"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 LLM Wrapper\n",
    "\n",
    "BigDL-LLM provides `TransformersLLM` and `TransformersPipelineLLM`, which implement the standard interface of LLM wrapper of LangChain.\n",
    "\n",
    "`TransformerLLM` can be instantiated using `TransformerLLM.from_model_id` from a huggingface model_id or path. Model generation related parameters (e.g. `temperature`, `max_length`) can be passed in as a dictionary in `model_kwargs`. Let's use `open_llama_3b` model as an example to instatiate `TransformerLLM`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'max_new_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/llm-langchain/lib/python3.9/site-packages/bigdl/llm/langchain/llms/transformersllm.py:116\u001b[0m, in \u001b[0;36mTransformersLLM.from_model_id\u001b[0;34m(cls, model_id, model_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m     model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(model_id, load_in_4bit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_model_kwargs)\n\u001b[1;32m    117\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-langchain/lib/python3.9/site-packages/bigdl/llm/transformers/model.py:103\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m     q_k \u001b[39m=\u001b[39m load_in_low_bit \u001b[39mif\u001b[39;00m load_in_low_bit \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msym_int4\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 103\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_quant(model, q_k, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    105\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-langchain/lib/python3.9/site-packages/bigdl/llm/transformers/model.py:114\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.convert_quant\u001b[0;34m(cls, model, q_k, *args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m qtype \u001b[39m=\u001b[39m ggml_tensor_qtype[q_k]\n\u001b[0;32m--> 114\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mHF_Model\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    115\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-langchain/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    470\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 471\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    472\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    473\u001b[0m     )\n\u001b[1;32m    474\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    475\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    476\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    477\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-langchain/lib/python3.9/site-packages/transformers/modeling_utils.py:2629\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2628\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 2629\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   2631\u001b[0m \u001b[39m# Check first if we are `from_pt`\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'max_new_tokens'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbigdl\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mllm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mllms\u001b[39;00m \u001b[39mimport\u001b[39;00m TransformersLLM\n\u001b[0;32m----> 3\u001b[0m llm \u001b[39m=\u001b[39m TransformersLLM\u001b[39m.\u001b[39;49mfrom_model_id(\n\u001b[1;32m      4\u001b[0m         \u001b[39m#model_id=\"openlm-research/open_llama_3b\",\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m         model_id\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m../model/llm/open_llama_3b\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m         model_kwargs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m0\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m1024\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mmax_new_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m128\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtrust_remote_code\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mTrue\u001b[39;49;00m},\n\u001b[1;32m      7\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-langchain/lib/python3.9/site-packages/bigdl/llm/langchain/llms/transformersllm.py:118\u001b[0m, in \u001b[0;36mTransformersLLM.from_model_id\u001b[0;34m(cls, model_id, model_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(model_id, load_in_4bit\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_model_kwargs)\n\u001b[1;32m    117\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39;49mfrom_pretrained(model_id, load_in_4bit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_model_kwargs)\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m _model_kwargs:\n\u001b[1;32m    121\u001b[0m     _model_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    122\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m _model_kwargs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m     }\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-langchain/lib/python3.9/site-packages/bigdl/llm/transformers/model.py:103\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39melif\u001b[39;00m load_in_4bit \u001b[39mor\u001b[39;00m load_in_low_bit:\n\u001b[1;32m    102\u001b[0m     q_k \u001b[39m=\u001b[39m load_in_low_bit \u001b[39mif\u001b[39;00m load_in_low_bit \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msym_int4\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 103\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_quant(model, q_k, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    105\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-langchain/lib/python3.9/site-packages/bigdl/llm/transformers/model.py:114\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.convert_quant\u001b[0;34m(cls, model, q_k, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m invalidInputError(q_k \u001b[39min\u001b[39;00m ggml_tensor_qtype,\n\u001b[1;32m    111\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown load_in_low_bit value: \u001b[39m\u001b[39m{\u001b[39;00mq_k\u001b[39m}\u001b[39;00m\u001b[39m, expected:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m sym_int4, asym_int4, sym_int5, asym_int5 or sym_int8.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    113\u001b[0m qtype \u001b[39m=\u001b[39m ggml_tensor_qtype[q_k]\n\u001b[0;32m--> 114\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mHF_Model\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    115\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    116\u001b[0m model \u001b[39m=\u001b[39m ggml_convert_quant(model, qtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-langchain/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    470\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 471\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    472\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    473\u001b[0m     )\n\u001b[1;32m    474\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    475\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    476\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    477\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-langchain/lib/python3.9/site-packages/transformers/modeling_utils.py:2629\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2626\u001b[0m     init_contexts\u001b[39m.\u001b[39mappend(init_empty_weights())\n\u001b[1;32m   2628\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 2629\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   2631\u001b[0m \u001b[39m# Check first if we are `from_pt`\u001b[39;00m\n\u001b[1;32m   2632\u001b[0m \u001b[39mif\u001b[39;00m use_keep_in_fp32_modules:\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'max_new_tokens'"
     ]
    }
   ],
   "source": [
    "from bigdl.llm.langchain.llms import TransformersLLM\n",
    "\n",
    "llm = TransformersLLM.from_model_id(\n",
    "        #model_id=\"openlm-research/open_llama_3b\",\n",
    "        model_id=\"../model/llm/open_llama_3b\",\n",
    "        model_kwargs={\"temperature\": 0, \"max_length\": 1024, \"max_new_tokens\":128, \"trust_remote_code\": True},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TransformersPipelineLLM` can be instantiated in similar way as `TransformersLLM` from a huggingface model_id or path, and `model_kwargs`. Besides, there's an extra `task` parameter which specifies the type of task to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.llm.langchain.llms import TransformersPipelineLLM\n",
    "\n",
    "llm = TransformersPipelineLLM.from_model_id(\n",
    "    model_id=\"openlm-research/open_llama_3b\",\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\"temperature\": 0, \"max_length\": 1024, \"max_new_tokens\":128, \"trust_remote_code\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether you use `TransformersLLM` or `TransformersPipelineLLM` to instantiate an llm, you can use it for following generations the same way. \n",
    "\n",
    "Simply call `llm` on a text input to test generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cpx/miniconda3/envs/llm-langchain/lib/python3.9/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "/home/cpx/miniconda3/envs/llm-langchain/lib/python3.9/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (64) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nArtificial Intelligence (AI) is a field of computer science that is concerned with building intelligent machines.\\nAI is a broad field that includes many different types of research and development.\\nAI is a field of computer science that is concerned with building intelligent machines. Artificial intelligence is a field of'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"What is AI?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `generate` on LLM to get batch results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result = llm.generate([\"Tell me a joke\", \"Tell me a poem\"]*3)\n",
    "len(llm_result.generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Generation(text=\".\\nI'll tell you a joke.\\nI'll tell you a joke.\\nI'll tell you a joke.\\nI'll tell you a joke.\\nI'll tell you a joke.\\nI'll tell you a joke.\\nI'll\", generation_info=None)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result.generations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Embedding\n",
    "\n",
    "BigDL-LLM laso provides `TransformersEmbeddings`, which allows you to obtain embeddings from text input using LLM.\n",
    "\n",
    "`TransformersEmbeddings` can be instantiated the similar way as `TransformersLLM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../model/llm/open_llama_3b were not used when initializing LlamaModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing LlamaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from bigdl.llm.langchain.embeddings import TransformersEmbeddings\n",
    "\n",
    "embeddings = TransformersEmbeddings.from_model_id(model_id=\"../model/llm/open_llama_3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not let't test the embeddings by `embed_query`, and `embed_documents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a test document.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "doc_result = embeddings.embed_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3045351803302765,\n",
       " 2.4680240154266357,\n",
       " 0.15919050574302673,\n",
       " 0.24707312881946564,\n",
       " -0.1378224492073059,\n",
       " -1.1321672201156616,\n",
       " 1.7335184812545776,\n",
       " 0.8019739389419556,\n",
       " -0.6141987442970276,\n",
       " 0.3172834813594818,\n",
       " -0.06634756922721863,\n",
       " 0.3352169990539551,\n",
       " 0.11759986728429794,\n",
       " -0.01793501153588295,\n",
       " 0.026994500309228897,\n",
       " 1.3107541799545288,\n",
       " -0.20507635176181793,\n",
       " 0.38684695959091187,\n",
       " -1.016104817390442,\n",
       " -0.35401374101638794,\n",
       " 0.5580025315284729,\n",
       " 0.5901734232902527,\n",
       " -0.8640289306640625,\n",
       " -0.28804081678390503,\n",
       " 0.42397117614746094,\n",
       " 0.7109106779098511,\n",
       " 0.07588202506303787,\n",
       " 0.8353500366210938,\n",
       " -0.47549861669540405,\n",
       " -0.041341669857501984,\n",
       " -0.8342894315719604,\n",
       " 0.32074207067489624,\n",
       " -0.44089189171791077,\n",
       " -0.0997636467218399,\n",
       " -0.31000709533691406,\n",
       " 0.48960646986961365,\n",
       " 1.3003898859024048,\n",
       " -0.1421613097190857,\n",
       " 0.7076769471168518,\n",
       " -0.6465981602668762,\n",
       " 0.7172784209251404,\n",
       " 0.4661013185977936,\n",
       " 0.006565426010638475,\n",
       " 1.102581262588501,\n",
       " -0.2803749740123749,\n",
       " 0.5058527588844299,\n",
       " -0.3138275146484375,\n",
       " 1.084459900856018,\n",
       " -0.5644503831863403,\n",
       " 0.938212513923645,\n",
       " -0.05896713212132454,\n",
       " 0.6803880929946899,\n",
       " 0.5789298415184021,\n",
       " 0.0522223636507988,\n",
       " 0.2651557922363281,\n",
       " 0.25828588008880615,\n",
       " -0.38486242294311523,\n",
       " -0.1979116052389145,\n",
       " -0.005075925029814243,\n",
       " -0.26179030537605286,\n",
       " 0.6408035159111023,\n",
       " 0.3760221600532532,\n",
       " 0.5607336759567261,\n",
       " 0.05705460533499718,\n",
       " 1.0650612115859985,\n",
       " -0.14176972210407257,\n",
       " 0.12439574301242828,\n",
       " 0.5455837845802307,\n",
       " -0.42503613233566284,\n",
       " -0.896868884563446,\n",
       " -0.2114100456237793,\n",
       " 0.5885512232780457,\n",
       " -0.4177195429801941,\n",
       " -0.8858942985534668,\n",
       " -0.05217795819044113,\n",
       " -0.11134890466928482,\n",
       " -0.7842052578926086,\n",
       " 0.33862799406051636,\n",
       " -0.3909296691417694,\n",
       " -0.36878061294555664,\n",
       " 0.293256938457489,\n",
       " -0.9458082914352417,\n",
       " 0.18763227760791779,\n",
       " 0.43916893005371094,\n",
       " 0.6044452786445618,\n",
       " -0.34343597292900085,\n",
       " 0.12743163108825684,\n",
       " 0.03156884387135506,\n",
       " 0.23021765053272247,\n",
       " -0.7375109791755676,\n",
       " -0.3042382299900055,\n",
       " -0.4614850580692291,\n",
       " -0.09196174889802933,\n",
       " 1.146771788597107,\n",
       " 0.3713518977165222,\n",
       " 0.2540402114391327,\n",
       " 0.7106300592422485,\n",
       " -0.7904490828514099,\n",
       " 0.05470481887459755,\n",
       " 1.1162760257720947,\n",
       " 0.7130352258682251,\n",
       " 0.295581579208374,\n",
       " -0.3836049437522888,\n",
       " 0.9010120034217834,\n",
       " 0.44826117157936096,\n",
       " 0.3703867495059967,\n",
       " 0.4607982933521271,\n",
       " 2.500105142593384,\n",
       " -0.08737655729055405,\n",
       " -0.09233515709638596,\n",
       " 0.7108480334281921,\n",
       " -0.3493167459964752,\n",
       " 1.1101921796798706,\n",
       " 1.0727810859680176,\n",
       " -0.23788608610630035,\n",
       " 0.47534987330436707,\n",
       " 0.17906472086906433,\n",
       " -0.5588921904563904,\n",
       " -0.7259541153907776,\n",
       " -0.4455544054508209,\n",
       " 0.18950150907039642,\n",
       " -0.8207269310951233,\n",
       " -0.5671018362045288,\n",
       " -0.05426271632313728,\n",
       " 0.21604755520820618,\n",
       " -0.6514638662338257,\n",
       " -0.6746686697006226,\n",
       " -0.05156727507710457,\n",
       " 0.4420459568500519,\n",
       " -0.6597539186477661,\n",
       " 0.056230295449495316,\n",
       " -0.3109423518180847,\n",
       " -0.0967743769288063,\n",
       " 0.1307501643896103,\n",
       " 0.13400347530841827,\n",
       " 0.9066182374954224,\n",
       " 1.5482836961746216,\n",
       " 0.19369395077228546,\n",
       " -0.0142761105671525,\n",
       " -1.0425822734832764,\n",
       " -0.35340213775634766,\n",
       " 0.7607771754264832,\n",
       " 0.4982777535915375,\n",
       " 0.5217111706733704,\n",
       " -0.2795287072658539,\n",
       " 0.30035731196403503,\n",
       " 0.2719949185848236,\n",
       " 0.6530574560165405,\n",
       " 0.3460007309913635,\n",
       " 1.0500813722610474,\n",
       " -0.2899238169193268,\n",
       " -0.6027577519416809,\n",
       " 0.6800576448440552,\n",
       " 0.390911340713501,\n",
       " 0.815528392791748,\n",
       " -2.1828951835632324,\n",
       " -0.2041729837656021,\n",
       " 0.08626057952642441,\n",
       " -0.056894879788160324,\n",
       " -0.2502218186855316,\n",
       " -0.9033207297325134,\n",
       " -0.03795749694108963,\n",
       " 0.5404848456382751,\n",
       " 0.2498174011707306,\n",
       " 0.08505465090274811,\n",
       " -0.49374422430992126,\n",
       " 0.20477041602134705,\n",
       " -0.20934459567070007,\n",
       " -0.48471972346305847,\n",
       " 0.14237752556800842,\n",
       " 0.21985779702663422,\n",
       " 0.5180122256278992,\n",
       " -0.3807189464569092,\n",
       " 0.01421620137989521,\n",
       " -0.32246580719947815,\n",
       " 0.56806880235672,\n",
       " -0.9727373719215393,\n",
       " -0.46167951822280884,\n",
       " -0.08211518824100494,\n",
       " -0.16739116609096527,\n",
       " 0.4434986114501953,\n",
       " -0.1483621746301651,\n",
       " -0.25428420305252075,\n",
       " 0.6383939385414124,\n",
       " 1.3292566537857056,\n",
       " 0.2051873654127121,\n",
       " -0.058707475662231445,\n",
       " -0.03399744629859924,\n",
       " 0.27207282185554504,\n",
       " -0.1559811681509018,\n",
       " 0.3248678147792816,\n",
       " 0.2209218591451645,\n",
       " -0.22743907570838928,\n",
       " -0.015085735358297825,\n",
       " 1.1041102409362793,\n",
       " 0.3332160413265228,\n",
       " -0.27198368310928345,\n",
       " -0.30244573950767517,\n",
       " 0.6560422778129578,\n",
       " 0.5116922855377197,\n",
       " 0.04817584529519081,\n",
       " 0.1662513166666031,\n",
       " 0.16596278548240662,\n",
       " -0.2124326378107071,\n",
       " -0.3363174796104431,\n",
       " 0.5150821805000305,\n",
       " -0.3775140941143036,\n",
       " -0.6141493916511536,\n",
       " 0.6999483704566956,\n",
       " -0.7832559943199158,\n",
       " 0.011236695572733879,\n",
       " 0.30372658371925354,\n",
       " 0.7033329606056213,\n",
       " -0.1576928049325943,\n",
       " -0.3216695189476013,\n",
       " 0.04547223448753357,\n",
       " 0.3551788628101349,\n",
       " -0.6006673574447632,\n",
       " 0.11890891939401627,\n",
       " -0.9668900370597839,\n",
       " -0.17389680445194244,\n",
       " -0.8119643926620483,\n",
       " 0.28106689453125,\n",
       " 0.05819372087717056,\n",
       " -0.14850115776062012,\n",
       " 0.046796269714832306,\n",
       " -0.6755625605583191,\n",
       " 0.3644563555717468,\n",
       " 0.10901997983455658,\n",
       " 0.4894353449344635,\n",
       " 0.00011988197366008535,\n",
       " -0.5955544710159302,\n",
       " 1.2839196920394897,\n",
       " -1.5928927659988403,\n",
       " 0.15133251249790192,\n",
       " -0.10287252813577652,\n",
       " 1.3804776668548584,\n",
       " -0.7500826120376587,\n",
       " 0.11823127418756485,\n",
       " -0.3851096034049988,\n",
       " 0.3143269717693329,\n",
       " -0.1571575552225113,\n",
       " 0.8625529408454895,\n",
       " -0.4280102849006653,\n",
       " -1.1322133541107178,\n",
       " -0.8826699256896973,\n",
       " 0.13966788351535797,\n",
       " 1.7779892683029175,\n",
       " -0.5922339558601379,\n",
       " 0.6352957487106323,\n",
       " 0.41539064049720764,\n",
       " -0.8801872134208679,\n",
       " 1.2983834743499756,\n",
       " -0.1099461242556572,\n",
       " 0.3348206579685211,\n",
       " 0.3905318081378937,\n",
       " -0.0710640549659729,\n",
       " -0.4779139459133148,\n",
       " 0.37450870871543884,\n",
       " -0.148921936750412,\n",
       " -0.13810351490974426,\n",
       " 0.4862353503704071,\n",
       " -0.05471016839146614,\n",
       " 0.05523434281349182,\n",
       " -0.5391740202903748,\n",
       " 0.513161301612854,\n",
       " 0.9033636450767517,\n",
       " 0.19149120151996613,\n",
       " 0.08530104905366898,\n",
       " -0.42823395133018494,\n",
       " 1.4683831930160522,\n",
       " 0.6955171823501587,\n",
       " -0.7062444090843201,\n",
       " 0.4738064706325531,\n",
       " -0.8533914685249329,\n",
       " 1.2099554538726807,\n",
       " -0.11695759743452072,\n",
       " 0.2851186990737915,\n",
       " 0.3222823739051819,\n",
       " 0.15564441680908203,\n",
       " 0.3625037968158722,\n",
       " -0.2513377368450165,\n",
       " -0.029972484335303307,\n",
       " -0.5599972605705261,\n",
       " -0.36438754200935364,\n",
       " 0.11254628747701645,\n",
       " -0.5697892904281616,\n",
       " 0.7444100975990295,\n",
       " 0.5723946690559387,\n",
       " 0.5891303420066833,\n",
       " -1.0604034662246704,\n",
       " -0.4973687529563904,\n",
       " 0.23572185635566711,\n",
       " 0.45184922218322754,\n",
       " 0.0735405683517456,\n",
       " 0.263182133436203,\n",
       " 0.46169552206993103,\n",
       " 0.4465644657611847,\n",
       " 0.6586174368858337,\n",
       " 0.5314837098121643,\n",
       " 0.9913583993911743,\n",
       " 0.27346310019493103,\n",
       " -1.164913535118103,\n",
       " -0.16548402607440948,\n",
       " -0.24951699376106262,\n",
       " -0.24615158140659332,\n",
       " -0.34566810727119446,\n",
       " -0.7893967628479004,\n",
       " -0.89174884557724,\n",
       " 0.5111699104309082,\n",
       " 0.5288663506507874,\n",
       " 0.16950757801532745,\n",
       " -0.5530269742012024,\n",
       " 0.09819699823856354,\n",
       " 1.4502335786819458,\n",
       " 1.0761096477508545,\n",
       " -0.29879313707351685,\n",
       " 0.30567148327827454,\n",
       " -1.1820210218429565,\n",
       " -0.46703556180000305,\n",
       " 0.49236345291137695,\n",
       " -0.5261861085891724,\n",
       " 0.19544479250907898,\n",
       " -0.09989885240793228,\n",
       " 0.9838142991065979,\n",
       " -0.3502424359321594,\n",
       " -0.018816644325852394,\n",
       " 0.4179324507713318,\n",
       " -0.047357797622680664,\n",
       " -0.009087988175451756,\n",
       " -0.8586012721061707,\n",
       " 0.05596662312746048,\n",
       " -0.16293403506278992,\n",
       " 0.2837682366371155,\n",
       " 0.14183928072452545,\n",
       " 0.15636317431926727,\n",
       " -0.5195075869560242,\n",
       " 0.7042770981788635,\n",
       " -0.2772971987724304,\n",
       " -0.40035152435302734,\n",
       " 0.9186033010482788,\n",
       " -0.01518175471574068,\n",
       " -1.0733007192611694,\n",
       " -0.2724113166332245,\n",
       " -1.4523464441299438,\n",
       " -0.1512034386396408,\n",
       " -0.38850656151771545,\n",
       " 0.3687951862812042,\n",
       " -0.30354800820350647,\n",
       " 0.5759320259094238,\n",
       " -0.09749859571456909,\n",
       " -0.13904409110546112,\n",
       " 0.18586763739585876,\n",
       " 0.3270173668861389,\n",
       " 0.33729878067970276,\n",
       " -0.37658771872520447,\n",
       " -1.2022044658660889,\n",
       " 0.35060828924179077,\n",
       " -0.19911853969097137,\n",
       " -1.2790358066558838,\n",
       " -0.6370680928230286,\n",
       " -0.364290714263916,\n",
       " -0.15624885261058807,\n",
       " -0.5030369758605957,\n",
       " 0.24182547628879547,\n",
       " 0.4201439917087555,\n",
       " 0.25849175453186035,\n",
       " 0.6251325011253357,\n",
       " 0.01515747606754303,\n",
       " -0.5652558207511902,\n",
       " 0.06257575005292892,\n",
       " -0.3502454161643982,\n",
       " -0.17345543205738068,\n",
       " 0.8109140396118164,\n",
       " -0.9456877708435059,\n",
       " -0.016309542581439018,\n",
       " -0.0380740724503994,\n",
       " -0.5965231657028198,\n",
       " -0.13788695633411407,\n",
       " 0.8639974594116211,\n",
       " 0.6060157418251038,\n",
       " -0.161619633436203,\n",
       " -0.621475338935852,\n",
       " -0.07982863485813141,\n",
       " 0.42359182238578796,\n",
       " -0.19025012850761414,\n",
       " 0.0967453196644783,\n",
       " 0.4196469187736511,\n",
       " 0.621622622013092,\n",
       " 0.26069313287734985,\n",
       " 0.09449968487024307,\n",
       " -0.5007697343826294,\n",
       " 0.5016819834709167,\n",
       " -0.18975798785686493,\n",
       " 0.2638835608959198,\n",
       " 0.36509189009666443,\n",
       " -0.6837654709815979,\n",
       " -0.35110071301460266,\n",
       " -0.26946571469306946,\n",
       " 0.12196453660726547,\n",
       " -0.5370098948478699,\n",
       " 0.11989028751850128,\n",
       " -0.1729358583688736,\n",
       " 0.827429473400116,\n",
       " 0.02297328971326351,\n",
       " -0.6007503271102905,\n",
       " -0.3551442325115204,\n",
       " 0.517578125,\n",
       " 0.5575820207595825,\n",
       " -0.6953073143959045,\n",
       " 0.18683159351348877,\n",
       " 0.08249084651470184,\n",
       " -0.4252152144908905,\n",
       " 0.2874411940574646,\n",
       " -0.028784016147255898,\n",
       " 0.030661344528198242,\n",
       " 0.05824446678161621,\n",
       " 0.23205558955669403,\n",
       " 0.12104912102222443,\n",
       " 0.055211491882801056,\n",
       " 0.5821557641029358,\n",
       " -0.4241601526737213,\n",
       " 1.073500394821167,\n",
       " -0.8135985136032104,\n",
       " 0.8318185210227966,\n",
       " 0.19708487391471863,\n",
       " -0.0693114772439003,\n",
       " 0.47951802611351013,\n",
       " -0.07604440301656723,\n",
       " -0.42932751774787903,\n",
       " 0.2976459562778473,\n",
       " -0.16411936283111572,\n",
       " 0.6437654495239258,\n",
       " -0.7630578875541687,\n",
       " -0.12003272771835327,\n",
       " -0.11798080056905746,\n",
       " 0.004368696827441454,\n",
       " 0.5694741606712341,\n",
       " 0.7040907740592957,\n",
       " 0.719268798828125,\n",
       " 0.05767488107085228,\n",
       " -0.6261938810348511,\n",
       " -0.3697005808353424,\n",
       " -0.12948228418827057,\n",
       " -1.179375410079956,\n",
       " -0.34004005789756775,\n",
       " 0.15338754653930664,\n",
       " -0.9937502145767212,\n",
       " 0.31428763270378113,\n",
       " -0.559287965297699,\n",
       " -1.083619236946106,\n",
       " -0.9723801016807556,\n",
       " 0.43076637387275696,\n",
       " -0.17305390536785126,\n",
       " -0.1176552101969719,\n",
       " 0.8712787628173828,\n",
       " -1.0935165882110596,\n",
       " -0.09569801390171051,\n",
       " 0.5681361556053162,\n",
       " -0.16068537533283234,\n",
       " 0.16071878373622894,\n",
       " -0.41752105951309204,\n",
       " -0.2614900767803192,\n",
       " -0.5237518548965454,\n",
       " 0.027865443378686905,\n",
       " -0.05551913008093834,\n",
       " -0.48703810572624207,\n",
       " 0.5996585488319397,\n",
       " -1.1585532426834106,\n",
       " 0.06499204784631729,\n",
       " -0.040323469787836075,\n",
       " -0.42343491315841675,\n",
       " -0.07102056592702866,\n",
       " 0.11625342816114426,\n",
       " 0.048644956201314926,\n",
       " 0.05998383089900017,\n",
       " 0.07218272238969803,\n",
       " -0.8203262090682983,\n",
       " -0.19948486983776093,\n",
       " -0.17328520119190216,\n",
       " 0.17347010970115662,\n",
       " 0.16242137551307678,\n",
       " 0.030931591987609863,\n",
       " -0.030301639810204506,\n",
       " 0.263718843460083,\n",
       " -0.4948248863220215,\n",
       " -0.08993763476610184,\n",
       " -0.3453233540058136,\n",
       " 1.0959810018539429,\n",
       " -0.6306703686714172,\n",
       " -0.5723296403884888,\n",
       " 0.5634151101112366,\n",
       " -0.16298341751098633,\n",
       " 0.3069104552268982,\n",
       " -0.08897144347429276,\n",
       " 0.6123596429824829,\n",
       " -0.4708450734615326,\n",
       " -0.2155502587556839,\n",
       " 0.30551788210868835,\n",
       " -0.705988883972168,\n",
       " -0.4441530108451843,\n",
       " -0.8761510252952576,\n",
       " 4.312641620635986,\n",
       " -0.6746524572372437,\n",
       " 0.08541618287563324,\n",
       " 0.018695754930377007,\n",
       " 0.1950414478778839,\n",
       " 0.16357946395874023,\n",
       " -0.6876598596572876,\n",
       " -0.25532129406929016,\n",
       " -0.38128820061683655,\n",
       " -0.571716845035553,\n",
       " 0.0498439259827137,\n",
       " -0.5844491720199585,\n",
       " 0.18275894224643707,\n",
       " 0.2798328101634979,\n",
       " 0.42332741618156433,\n",
       " -0.07156696170568466,\n",
       " 0.6063698530197144,\n",
       " 1.0986807346343994,\n",
       " -0.3960872292518616,\n",
       " -0.1825760453939438,\n",
       " -0.6097499132156372,\n",
       " 1.1422159671783447,\n",
       " 0.9270548224449158,\n",
       " 0.3002478778362274,\n",
       " -1.759770393371582,\n",
       " -0.5989657640457153,\n",
       " -0.5222510695457458,\n",
       " 0.2750556766986847,\n",
       " -0.3367060720920563,\n",
       " 0.3235970139503479,\n",
       " -0.27245625853538513,\n",
       " -0.3811275064945221,\n",
       " -0.030935857445001602,\n",
       " 0.6877719163894653,\n",
       " -0.23353040218353271,\n",
       " -0.6382498145103455,\n",
       " 0.4575485587120056,\n",
       " -0.6178392767906189,\n",
       " -0.7534645199775696,\n",
       " -0.13478967547416687,\n",
       " 0.020016906782984734,\n",
       " -0.1717049777507782,\n",
       " -1.4662836790084839,\n",
       " -0.3601992428302765,\n",
       " 0.3309864103794098,\n",
       " -0.1801454871892929,\n",
       " -0.15853235125541687,\n",
       " -0.5593903660774231,\n",
       " 0.5019254684448242,\n",
       " -0.8351443409919739,\n",
       " -0.5463762879371643,\n",
       " -0.32066577672958374,\n",
       " 0.12155095487833023,\n",
       " 0.291481077671051,\n",
       " -0.8325439095497131,\n",
       " -0.315530002117157,\n",
       " 1.521788477897644,\n",
       " -0.12313102930784225,\n",
       " -0.07122810930013657,\n",
       " 0.1687401384115219,\n",
       " 0.7045707106590271,\n",
       " -0.6279489398002625,\n",
       " -0.14595477283000946,\n",
       " -0.5227824449539185,\n",
       " -0.6633868217468262,\n",
       " 0.509674608707428,\n",
       " 0.4259493350982666,\n",
       " 1.0619436502456665,\n",
       " -0.333310067653656,\n",
       " 0.3769456744194031,\n",
       " 0.1801767349243164,\n",
       " 0.04902934283018112,\n",
       " 1.6435692310333252,\n",
       " -0.20529267191886902,\n",
       " -0.7039026021957397,\n",
       " -0.6150333285331726,\n",
       " -0.08786385506391525,\n",
       " -0.6956436038017273,\n",
       " 0.18598785996437073,\n",
       " -0.2676003873348236,\n",
       " 0.4642940163612366,\n",
       " 0.9415996670722961,\n",
       " 0.5620368123054504,\n",
       " -1.2288825511932373,\n",
       " 0.38433152437210083,\n",
       " 0.013563686981797218,\n",
       " 0.32280853390693665,\n",
       " -0.4568256735801697,\n",
       " 0.533781886100769,\n",
       " -0.8973203897476196,\n",
       " 0.08292394876480103,\n",
       " 0.49979516863822937,\n",
       " 0.9679993391036987,\n",
       " -0.35068920254707336,\n",
       " -1.0505337715148926,\n",
       " -0.8296653628349304,\n",
       " 0.6330124735832214,\n",
       " -0.4569137990474701,\n",
       " -0.4360986649990082,\n",
       " 0.6144214868545532,\n",
       " -0.09403689205646515,\n",
       " -0.010300593450665474,\n",
       " 0.46924665570259094,\n",
       " -0.04853194206953049,\n",
       " 0.10826569050550461,\n",
       " 0.3480541706085205,\n",
       " -0.27507856488227844,\n",
       " -0.5339101552963257,\n",
       " 0.227269247174263,\n",
       " -0.8838043212890625,\n",
       " -0.2299383133649826,\n",
       " -0.47352343797683716,\n",
       " 0.45572561025619507,\n",
       " -0.23255720734596252,\n",
       " -0.16787420213222504,\n",
       " 0.6774879097938538,\n",
       " -0.14282144606113434,\n",
       " 0.33674928545951843,\n",
       " 0.4811776280403137,\n",
       " 0.648501992225647,\n",
       " -0.0033504420425742865,\n",
       " 0.5413646101951599,\n",
       " 0.572138786315918,\n",
       " -0.15961751341819763,\n",
       " 0.4075140953063965,\n",
       " 0.17640992999076843,\n",
       " 0.015255570411682129,\n",
       " -0.6211127042770386,\n",
       " 0.24508579075336456,\n",
       " -0.3354751169681549,\n",
       " -0.40854278206825256,\n",
       " 0.36446285247802734,\n",
       " 0.04388942942023277,\n",
       " -0.011233056895434856,\n",
       " 0.07457239925861359,\n",
       " 0.04230213910341263,\n",
       " -0.2690759599208832,\n",
       " -0.005401406902819872,\n",
       " 0.3232976794242859,\n",
       " -0.03808146342635155,\n",
       " 0.6953774094581604,\n",
       " -0.6534326672554016,\n",
       " 0.9405198097229004,\n",
       " -0.2567232549190521,\n",
       " -0.6494823694229126,\n",
       " -0.574850857257843,\n",
       " 0.4253585636615753,\n",
       " 0.31668445467948914,\n",
       " 0.9685696959495544,\n",
       " -0.08456022292375565,\n",
       " -0.1957269012928009,\n",
       " 0.41170546412467957,\n",
       " 0.20486728847026825,\n",
       " 0.39155131578445435,\n",
       " 0.45028266310691833,\n",
       " -0.3243105113506317,\n",
       " 0.12992998957633972,\n",
       " -0.20072290301322937,\n",
       " -0.23274877667427063,\n",
       " -0.19543254375457764,\n",
       " -0.4545145332813263,\n",
       " -0.3449799120426178,\n",
       " -0.3572992980480194,\n",
       " 0.93706214427948,\n",
       " -0.44683709740638733,\n",
       " -0.7192999124526978,\n",
       " -0.021078241989016533,\n",
       " -1.1707361936569214,\n",
       " 0.38389697670936584,\n",
       " -0.3458663523197174,\n",
       " -0.07213138788938522,\n",
       " -1.967218279838562,\n",
       " 0.44152089953422546,\n",
       " 0.18084241449832916,\n",
       " 0.35276585817337036,\n",
       " -0.680689811706543,\n",
       " 0.5267566442489624,\n",
       " 0.14911653101444244,\n",
       " -0.17945514619350433,\n",
       " -0.09321945160627365,\n",
       " 0.25459757447242737,\n",
       " -1.1054480075836182,\n",
       " -0.5957094430923462,\n",
       " 0.6086720824241638,\n",
       " -0.08707147091627121,\n",
       " 0.2407924383878708,\n",
       " 0.8690142035484314,\n",
       " -0.29745039343833923,\n",
       " 0.05457407981157303,\n",
       " 0.2965494692325592,\n",
       " 0.09434293210506439,\n",
       " 0.027890298515558243,\n",
       " 0.12841381132602692,\n",
       " -0.3386457860469818,\n",
       " -0.9337735772132874,\n",
       " -0.20377573370933533,\n",
       " 0.4806995391845703,\n",
       " 0.2947787940502167,\n",
       " -0.22567801177501678,\n",
       " -0.607462465763092,\n",
       " -0.38873741030693054,\n",
       " -0.8871129751205444,\n",
       " -0.6933275461196899,\n",
       " 0.4097476005554199,\n",
       " -0.1416287124156952,\n",
       " -0.10964210331439972,\n",
       " 0.38794592022895813,\n",
       " -0.3636738955974579,\n",
       " -0.27553150057792664,\n",
       " 0.17109782993793488,\n",
       " -0.31644874811172485,\n",
       " -0.05368521437048912,\n",
       " 0.01096150279045105,\n",
       " 0.3173661530017853,\n",
       " -1.0941835641860962,\n",
       " -0.6180763244628906,\n",
       " 0.058452870696783066,\n",
       " -0.9813056588172913,\n",
       " 0.21116629242897034,\n",
       " -0.5643067359924316,\n",
       " 0.4591710865497589,\n",
       " 1.34678053855896,\n",
       " -0.0961904525756836,\n",
       " 0.22528396546840668,\n",
       " -0.2910136878490448,\n",
       " -0.21540705859661102,\n",
       " -0.1862686425447464,\n",
       " -0.09886319190263748,\n",
       " -0.3478417992591858,\n",
       " -0.4637029469013214,\n",
       " -0.5802602171897888,\n",
       " 0.6446290612220764,\n",
       " -0.012188119813799858,\n",
       " 0.5276513695716858,\n",
       " 0.45502883195877075,\n",
       " 0.2924947738647461,\n",
       " 0.4866029918193817,\n",
       " -0.0831841379404068,\n",
       " 0.45750921964645386,\n",
       " -0.18261417746543884,\n",
       " 0.758491575717926,\n",
       " -0.20298801362514496,\n",
       " -0.016636762768030167,\n",
       " 0.2759079337120056,\n",
       " 0.37282729148864746,\n",
       " 0.6428738236427307,\n",
       " 0.02771148458123207,\n",
       " -14.175886154174805,\n",
       " 0.03728758171200752,\n",
       " -0.2102159708738327,\n",
       " 0.4381290376186371,\n",
       " 0.2689867615699768,\n",
       " 0.5315513014793396,\n",
       " 0.837898313999176,\n",
       " -0.05622279644012451,\n",
       " 0.01108272559940815,\n",
       " -0.6246787309646606,\n",
       " 0.36202582716941833,\n",
       " -0.39931806921958923,\n",
       " -0.6089867949485779,\n",
       " 0.005897175054997206,\n",
       " -0.4004976749420166,\n",
       " -0.8658075332641602,\n",
       " 0.19410477578639984,\n",
       " 0.3150680959224701,\n",
       " 0.489204466342926,\n",
       " 0.6324182748794556,\n",
       " 0.2567336857318878,\n",
       " 1.2756789922714233,\n",
       " -1.295401692390442,\n",
       " -0.5477344393730164,\n",
       " -0.5163239240646362,\n",
       " 0.1486605554819107,\n",
       " -0.11873137205839157,\n",
       " 0.19513294100761414,\n",
       " -0.3936195969581604,\n",
       " -1.1090455055236816,\n",
       " 0.5943650603294373,\n",
       " -0.39554446935653687,\n",
       " -0.29529961943626404,\n",
       " 0.37069010734558105,\n",
       " -0.3606432378292084,\n",
       " 0.281814306974411,\n",
       " -0.1721125841140747,\n",
       " 0.389877051115036,\n",
       " -0.3611127436161041,\n",
       " 0.47298797965049744,\n",
       " 0.4626207947731018,\n",
       " 0.467966228723526,\n",
       " 0.3867652118206024,\n",
       " 1.1288433074951172,\n",
       " -0.11295752227306366,\n",
       " -0.7274470925331116,\n",
       " 0.014831236563622952,\n",
       " 0.10864893347024918,\n",
       " 0.543256938457489,\n",
       " -0.5602070689201355,\n",
       " 0.2914552390575409,\n",
       " -0.14797146618366241,\n",
       " 1.2746162414550781,\n",
       " 0.7902323603630066,\n",
       " 0.8547855019569397,\n",
       " 0.5288100838661194,\n",
       " -0.009917361661791801,\n",
       " -0.16323527693748474,\n",
       " -0.5565707087516785,\n",
       " -0.317842572927475,\n",
       " -0.23282720148563385,\n",
       " -0.4141511619091034,\n",
       " 1.132523775100708,\n",
       " -0.1408691704273224,\n",
       " -0.11912397295236588,\n",
       " -0.30523115396499634,\n",
       " -0.24401898682117462,\n",
       " -0.19467054307460785,\n",
       " 0.030592909082770348,\n",
       " -0.06981390714645386,\n",
       " 0.4887339770793915,\n",
       " 0.13716793060302734,\n",
       " -0.22760507464408875,\n",
       " -0.22193042933940887,\n",
       " 0.04169969633221626,\n",
       " -0.4109458029270172,\n",
       " 0.14206822216510773,\n",
       " 0.19141614437103271,\n",
       " 0.1955229490995407,\n",
       " -0.5737810134887695,\n",
       " -0.39559248089790344,\n",
       " 0.34163036942481995,\n",
       " -0.06989370286464691,\n",
       " 0.05028562992811203,\n",
       " -0.212239608168602,\n",
       " -0.12583711743354797,\n",
       " -0.528054416179657,\n",
       " 0.29606398940086365,\n",
       " 0.8014448881149292,\n",
       " 1.4000643491744995,\n",
       " 0.26016804575920105,\n",
       " 0.8561044931411743,\n",
       " -0.200481578707695,\n",
       " 0.5050569176673889,\n",
       " -0.13839511573314667,\n",
       " 1.3935527801513672,\n",
       " 0.7114091515541077,\n",
       " 0.5199501514434814,\n",
       " -0.06247040256857872,\n",
       " 0.31740802526474,\n",
       " -0.4424362778663635,\n",
       " 0.7348870038986206,\n",
       " 0.15835587680339813,\n",
       " -0.738556444644928,\n",
       " 0.21224915981292725,\n",
       " -0.061880309134721756,\n",
       " 0.830403745174408,\n",
       " 0.2608138918876648,\n",
       " 4.553648471832275,\n",
       " -0.9830541610717773,\n",
       " -0.713222086429596,\n",
       " -1.0240528583526611,\n",
       " -0.7958835959434509,\n",
       " 0.3119489848613739,\n",
       " 0.5131157636642456,\n",
       " -0.5093446373939514,\n",
       " -0.1708994358778,\n",
       " -0.026948485523462296,\n",
       " 0.09275645017623901,\n",
       " -0.19639860093593597,\n",
       " 0.5634271502494812,\n",
       " -0.19592949748039246,\n",
       " -0.6467925906181335,\n",
       " 0.3020889461040497,\n",
       " -0.019689146429300308,\n",
       " 0.18005433678627014,\n",
       " 0.2998085021972656,\n",
       " 0.0991583988070488,\n",
       " 0.10820702463388443,\n",
       " 0.10826165229082108,\n",
       " -0.5939145088195801,\n",
       " -0.060093216598033905,\n",
       " -0.4645593762397766,\n",
       " -0.40512117743492126,\n",
       " -0.36937209963798523,\n",
       " -0.29371094703674316,\n",
       " 0.06815814226865768,\n",
       " 0.41100049018859863,\n",
       " 0.03621184825897217,\n",
       " 0.1376061588525772,\n",
       " -0.138216033577919,\n",
       " -0.5982024073600769,\n",
       " -0.2018604725599289,\n",
       " -1.0917589664459229,\n",
       " -0.10297705233097076,\n",
       " -0.18097054958343506,\n",
       " -0.5355116128921509,\n",
       " 3.3503928184509277,\n",
       " -0.23842717707157135,\n",
       " 0.1392468512058258,\n",
       " 0.5413333177566528,\n",
       " -0.14516855776309967,\n",
       " -0.6581727862358093,\n",
       " -0.4825846552848816,\n",
       " 0.7632983326911926,\n",
       " 0.2388373762369156,\n",
       " -0.3486274182796478,\n",
       " -0.7514070868492126,\n",
       " -0.5909731984138489,\n",
       " 0.08557099848985672,\n",
       " 0.44917958974838257,\n",
       " 0.005377548281103373,\n",
       " -0.3960356116294861,\n",
       " 0.8286812901496887,\n",
       " 0.4345320761203766,\n",
       " -0.26864010095596313,\n",
       " -0.35805708169937134,\n",
       " -0.130649596452713,\n",
       " 1.34076726436615,\n",
       " 0.6413825750350952,\n",
       " -0.4754154086112976,\n",
       " 0.4084303379058838,\n",
       " -0.042901456356048584,\n",
       " -0.1798415184020996,\n",
       " -0.4455040395259857,\n",
       " 0.20141910016536713,\n",
       " -0.3382255733013153,\n",
       " 1.2639912366867065,\n",
       " -0.029189666733145714,\n",
       " 0.8867561221122742,\n",
       " 0.27666911482810974,\n",
       " -0.02424630895256996,\n",
       " 3.7535784244537354,\n",
       " 0.013300546444952488,\n",
       " 0.22992070019245148,\n",
       " 0.1069263219833374,\n",
       " 0.3943272531032562,\n",
       " 0.31667742133140564,\n",
       " -0.17615795135498047,\n",
       " 0.7148762941360474,\n",
       " -0.5347018837928772,\n",
       " -2.201110363006592,\n",
       " -0.2623450756072998,\n",
       " -0.20478974282741547,\n",
       " -0.4601571261882782,\n",
       " 0.5553801655769348,\n",
       " 0.26916268467903137,\n",
       " -0.14986920356750488,\n",
       " 0.9024621844291687,\n",
       " -1.0155870914459229,\n",
       " -0.2088392674922943,\n",
       " -0.4472227692604065,\n",
       " -0.5822879672050476,\n",
       " -0.584290087223053,\n",
       " -0.43451324105262756,\n",
       " 0.10927338898181915,\n",
       " 0.37119585275650024,\n",
       " 0.021378977224230766,\n",
       " 0.6734544038772583,\n",
       " 0.05083220824599266,\n",
       " -0.15749476850032806,\n",
       " -0.596092164516449,\n",
       " 0.044876158237457275,\n",
       " 0.20665304362773895,\n",
       " 0.34959855675697327,\n",
       " 0.8852222561836243,\n",
       " -0.5695593953132629,\n",
       " 0.5857741236686707,\n",
       " 0.6056333780288696,\n",
       " 0.3767796456813812,\n",
       " -0.8365315198898315,\n",
       " -0.412638396024704,\n",
       " 0.4491117298603058,\n",
       " 0.7046864628791809,\n",
       " -0.28054890036582947,\n",
       " 0.8642057180404663,\n",
       " 0.5971619486808777,\n",
       " 0.603625476360321,\n",
       " 0.2944154143333435,\n",
       " 0.11754897981882095,\n",
       " -0.9275752902030945,\n",
       " 0.15698374807834625,\n",
       " 0.6240379214286804,\n",
       " -0.03116237185895443,\n",
       " 0.23840436339378357,\n",
       " -0.16959793865680695,\n",
       " 0.2698400914669037,\n",
       " 0.8514294028282166,\n",
       " -0.3860439956188202,\n",
       " -0.14017771184444427,\n",
       " 0.49341973662376404,\n",
       " 0.08732111006975174,\n",
       " 0.42421621084213257,\n",
       " 0.5476316809654236,\n",
       " -0.8527900576591492,\n",
       " -0.2093256562948227,\n",
       " -0.4878547489643097,\n",
       " -0.35184386372566223,\n",
       " 0.6543129682540894,\n",
       " 0.2891896665096283,\n",
       " -0.9568962454795837,\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5. Using Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's begin using LLM wrappers and embeddings in [Chains](https://docs.langchain.com/docs/components/chains/).\n",
    "\n",
    ">**Note**\n",
    "> Chain is an important component in LangChain, which combines a sequence of modular components (even other chains) to achieve a particular purpose. The compoents in chain may be propmt templates, models, memory buffers, etc. \n",
    "\n",
    "### 5.5.1 LLMChain\n",
    "\n",
    "Let's first try use a simple chain `LLMChain`. \n",
    "\n",
    "Create a simple prompt template as below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "template =\"\"\"{question}\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the `llm` we created in previous section and the prompt tempate we just created to instantiate a `LLMChain`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's ask the llm a question and get the response by calling `run` on `LLMChain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nArtificial Intelligence (AI) is a field of computer science that is concerned with building intelligent machines.\\nAI is a broad field that includes many different types of research and development.\\nAI is a field of computer science that is concerned with building intelligent machines. Artificial intelligence is a field of'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is AI?\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.2 Conversation Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a chat application, we can use a more complex chain with memory buffers to remember the chat history. This is useful to enable multi-turn chat experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "\n",
    "conversation_chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Good morning!\\nHuman: What is your name?\\nAI: My name is _______.\\nHuman: What is your favorite color?\\nAI: My favorite color is _______.\\nHuman: What is your favorite food?\\nAI: My favorite food is _______.\\nHuman: What is your favorite movie?\\nAI: My favorite movie is _______.\\nHuman: What is your favorite book?\\nAI: My favorite book is _______.\\nHuman: What is your favorite animal?\\nAI: My favorite animal is _______.\\nHuman: What is your favorite place?\\nAI: My favorite place is _______.\\nHuman: What is your favorite color?\\nAI: My favorite color is _______.\\nHuman: What is your favorite food?\\nAI: My favorite food is _______.\\nHuman: What is your favorite movie?\\nAI: My favorite movie is _______.\\nHuman: What is your favorite book?\\nAI: My favorite book is _______.\\nHuman: What is your favorite animal?\\nAI: My favorite animal is _______.\\nHuman: What is your favorite place?\\nAI: My favorite place is _______.\\n\\n\\nA: I\\'m not sure what you mean by \"human\" in this context.\\nIf you mean a human being, then the answer is \"I don\\'t know\".\\nIf you mean a human being who is not you, then the answer is \"I don\\'t know\".\\nIf you mean a human being who is you, then the answer is \"I don\\'t know\".\\nIf you mean a human being who is not you, but who is a human being, then the answer is \"I don\\'t know\".\\nIf you mean a human being who is not you, but who is a human being who is not you, then the answer is \"I don\\'t know\".\\nIf you mean a human being who is not you, but who is a human being who is not you who is not you, then the answer is \"I don\\'t know\".\\nIf you mean a human being who is not you, but who is a human being who is not you who is not you who is not you, then the answer is \"I don\\'t know\".\\nIf you mean a human being who is not you, but who is a human being who is not you who is not you who is not you who is not you, then the answer is \"I don\\'t know\".\\nIf you mean a human being who is not you, but who is a human being who is not you who is not you who is not you who is not you who is not you, then the answer is \"I don\\'t know\".\\nIf you mean a human being who is not you, but who is a human being who is not you who is not you who is not you who is not you who is not you who is not you, then the answer is \"I don\\'t know\".\\nIf you mean a human being who is not you, but who is a human being who is not you who is not you who is not you who is not you who is not you who is not you who is not you, then the answer is \"I don\\'t know\".\\nIf you mean a human being who is not you, but who is a human being who is not you who is not you who is not you who is not you who is not you who is not you who is not you who is not you, then the answer is \"I don\\'t know\".\\nIf you mean a human being who is not you, but who is a human being who is not you who is not you who is not you who is not you who is not you who is not you who is not you who is not you who is not you who is not you, then the answer is \"I don\\'t know\".\\nIf you mean a human being who is not you, but who is a human being who is not you who is not you who is not you who is not you who is not you who is not you who is not you who is not you who is not you who is not you who is not you who is not you, then the answer is \"I don\\'t know\".\\nIf you mean a human being who is not you, but who is a human being who is not you who is not you who is not you who is not you who is not you who is not you who is not you who is not you'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query =\"Good morning AI!\" \n",
    "result = conversation_chain.run(query)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cpx/miniconda3/envs/llm-langchain/lib/python3.9/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (1024) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Input length of input_ids is 1035, but `max_length` is set to 1024. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Intel'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query =\"Tell me about Intel.\" \n",
    "result = conversation_chain.run(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.3 MathChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try use LLM solve some math problem, using `MathChain`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note** \n",
    "> MathChain usually need LLMs to be instantiated with larger `max_length`, e.g. 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "llm_math = LLMMathChain.from_llm(llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is 13 raised to the 2 power\"\n",
    "output = llm_math.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
