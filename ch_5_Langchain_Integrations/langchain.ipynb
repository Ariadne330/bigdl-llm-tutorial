{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5: Langchain Integrations \n",
    "\n",
    "[LangChain](https://python.langchain.com/docs/get_started/introduction.html) is a library for developing applications powered by language models. You can use langchain LLM wrappers provided by `bigdl-llm` the same way as [other wrappers](https://python.langchain.com/docs/integrations/llms/). \n",
    "\n",
    "This notebook goes over how to use langchain to interact with BigDL-LLM.\n",
    "\n",
    "## 5.1 Install BigDL-LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bigdl-llm[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Install LangChain and related dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain==0.0.184\n",
    "!pip install -U typing_extensions==4.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 LLM Wrapper\n",
    "\n",
    "BigDL-LLM provides `TransformersLLM` and `TransformersPipelineLLM`, which implement the standard interface of LLM wrapper of langchain.\n",
    "\n",
    "`TransformerLLM` can be instantiated from a huggingface model_id or path, and model generation config can be passed in `model_kwargs`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.llm.langchain.llms import TransformersLLM\n",
    "\n",
    "llm = TransformersLLM.from_model_id(\n",
    "        #model_id=\"openlm-research/open_llama_3b\",\n",
    "        model_id=\"../model/llm/open_llama_3b\",\n",
    "        model_kwargs={\"temperature\": 0, \"max_length\": 64, \"trust_remote_code\": True},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TransformersPipelineLLM` can be instantiated from a huggingface model_id or path and `model_kwargs`, plus `task` parameter which specifies the type of task to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.llm.langchain.llms import TransformersPipelineLLM\n",
    "\n",
    "llm = TransformersPipelineLLM.from_model_id(\n",
    "    model_id=model_path,\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\"temperature\": 0, \"max_length\": 64, \"trust_remote_code\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether you use `TransformersLLM` or `TransformersPipelineLLM` to instantiate llm, you can use it the same way. \n",
    "\n",
    "To get an inference result, you can directly call llm on a text input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cpx/miniconda3/envs/llm-langchain/lib/python3.9/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (64) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nArtificial Intelligence (AI) is a field of computer science that is concerned with building intelligent machines.\\nAI is a broad field that includes many different types of research and development.\\nAI is a field of computer science that is concerned with building intelligent machines. Artificial intelligence is a field of'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"What is AI?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you may use `generate` on LLM to get batch results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cpx/miniconda3/envs/llm-langchain/lib/python3.9/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (64) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result = llm.generate([\"Tell me a joke\", \"Tell me a poem\"]*3)\n",
    "len(llm_result.generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Generation(text=\".\\nI'll tell you a joke.\\nI'll tell you a joke.\\nI'll tell you a joke.\\nI'll tell you a joke.\\nI'll tell you a joke.\\nI'll tell you a joke.\\nI'll\", generation_info=None)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result.generations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Embedding\n",
    "\n",
    "BigDL-LLM laso provides `TransformersEmbeddings`, which allows you to extrat embeddings from an input using LLM. \n",
    "\n",
    "`TransformersEmbeddings` can be instantiated the similar way as `TransformersLLM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.llm.langchain.embeddings import TransformersEmbeddings\n",
    "\n",
    "embeddings = TransformersEmbeddings.from_model_id(model_id=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
