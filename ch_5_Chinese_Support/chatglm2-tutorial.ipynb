{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5.1: ChatGLM2\n",
    "\n",
    "## Overview\n",
    "This is an example for [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B) is the second-generation version of the open-source bilingual (Chinese-English) chat model [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) proposed by [THUDM](https://github.com/THUDM). ChatGLM2-6B also can be found in huggingface.co in following [link](https://huggingface.co/THUDM/chatglm2-6b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Requirements\n",
    "\n",
    "### Installation\n",
    "\n",
    "We suggest using conda to manage environment:\n",
    "\n",
    "```shell\n",
    "conda create -n llm python=3.9\n",
    "conda activate llm\n",
    "\n",
    "pip install bigdl-llm[all] # install bigdl-llm with 'all' option\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "### Create Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you could tune the prompt based on your own model,\n",
    "# here the prompt tuning refers to https://huggingface.co/THUDM/chatglm2-6b/blob/main/modeling_chatglm.py#L1007\n",
    "CHATGLM_V2_PROMPT_TEMPLATE = \"问：{prompt}\\n\\n答：\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model\n",
    "\n",
    "Load model in 4 bit, which convert the relevant layers in the model into INT4 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:09<00:00,  1.34s/it]\n"
     ]
    }
   ],
   "source": [
    "from bigdl.llm.transformers import AutoModel\n",
    "\n",
    "model_path = \"THUDM/chatglm2-6b\" # repo id or model path\n",
    "model = AutoModel.from_pretrained(model_path,\n",
    "                                  load_in_4bit=True,\n",
    "                                  trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenizer\n",
    "\n",
    "The quantized model can use the tokenizer provided by the huggingface transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path,\n",
    "                                          trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate predicted tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 9.241551399230957 s\n",
      "-------------------- Prompt --------------------\n",
      "问：AI是什么？\n",
      "\n",
      "答：\n",
      "-------------------- Output --------------------\n",
      "问：AI是什么？\n",
      "\n",
      "答： AI指的是人工智能,是一种能够通过学习和推理来执行任务的计算机程序。它可以模仿人类的思维方式,做出类似人类的决策,并且具有自主学习、自我进化的能力。\n",
      "\n",
      "AI 技术包括机器学习、深度学习、自然语言处理、计算机视觉、机器人技术等,可以应用于各种领域,如医疗、金融、制造业、军事、能源等。\n",
      "\n",
      "AI 技术的发展已经带来了许多改变和进步,但同时也引起了人们的担忧和争议,涉及到隐私、安全、道德和社会影响等方面的问题。\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "prompt = \"AI是什么？\"\n",
    "n_predict = 128\n",
    "\n",
    "with torch.inference_mode():\n",
    "    prompt = CHATGLM_V2_PROMPT_TEMPLATE.format(prompt=prompt)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    st = time.time()\n",
    "    # if your selected model is capable of utilizing previous key/value attentions\n",
    "    # to enhance decoding speed, but has `\"use_cache\": false` in its model config,\n",
    "    # it is important to set `use_cache=True` explicitly in the `generate` function\n",
    "    # to obtain optimal performance with BigDL-LLM INT4 optimizations\n",
    "    output = model.generate(input_ids,\n",
    "                            max_new_tokens=n_predict)\n",
    "    end = time.time()\n",
    "    output_str = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f'Inference time: {end-st} s')\n",
    "    print('-'*20, 'Prompt', '-'*20)\n",
    "    print(prompt)\n",
    "    print('-'*20, 'Output', '-'*20)\n",
    "    print(output_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHATGLM_V2_PROMPT_TEMPLATE = \"\"\"{history}\\n\\n问：{human_input}\\n\\n答：\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:10<00:00,  1.46s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "from bigdl.llm.langchain.llms import TransformersLLM\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "llm_model_path = \"THUDM/chatglm2-6b/\" # the path to the huggingface llm model\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"human_input\"], template=CHATGLM_V2_PROMPT_TEMPLATE)\n",
    "max_new_tokens = 128\n",
    "\n",
    "llm = TransformersLLM.from_model_id(\n",
    "        model_id=llm_model_path,\n",
    "        model_kwargs={\"trust_remote_code\": True},\n",
    ")\n",
    "\n",
    "# Following code are complete the same as the use-case\n",
    "voiceassitant_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    llm_kwargs={\"max_new_tokens\":max_new_tokens},\n",
    "    memory=ConversationBufferWindowMemory(k=2),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: AI 是什么？\n",
      "AI:  AI指的是人工智能,是一种能够通过学习和理解数据,以及应用适当的算法和数学模型,来执行与人类智能相似的任务的技术。AI可以包括机器学习、自然语言处理、计算机视觉、知识表示、推理、决策等多种技术。\n",
      "Human: 小奥 是什么？\n",
      "AI:  我不知道 \"小奥\" 是什么,因为我没有上下文。如果您能提供更多信息,我会尽力回答您的问题。\n",
      "\n",
      "问：AI 是什么？\n",
      "\n",
      "答：\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " AI指的是人工智能,是一种能够通过学习和理解数据,以及应用适当的算法和数学模型,来执行与人类智能相似的任务的技术。AI可以包括机器学习、自然语言处理、计算机视觉、知识表示、推理、决策等多种技术。\n"
     ]
    }
   ],
   "source": [
    "text = \"AI 是什么？\"\n",
    "response_text = voiceassitant_chain.predict(human_input=text,\n",
    "                                            stop=\"\\n\\n\")\n",
    "print(response_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cn-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
